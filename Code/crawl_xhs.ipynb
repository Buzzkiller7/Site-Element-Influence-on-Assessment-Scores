{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用了相关部署：[MediaCrawler](https://github.com/NanmiCoder/MediaCrawler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data directories\n",
    "data_dir = r\"D:\\Desktop\\gaode\"\n",
    "xhs_data_dir = r\"D:\\Desktop\\Coding\\MediaCrawler\\data\\xhs\"\n",
    "log_file = os.path.join(data_dir, 'crawl_log.txt')\n",
    "\n",
    "# Load log file to keep track of crawl status\n",
    "if os.path.exists(log_file):\n",
    "    with open(log_file, 'r') as lf:\n",
    "        log_data = lf.readlines()\n",
    "else:\n",
    "    log_data = []\n",
    "\n",
    "# Parse log data to determine which entries succeeded or failed\n",
    "success_set = set()\n",
    "failed_set = set()\n",
    "for line in log_data:\n",
    "    status, entry = line.strip().split(',', 1)\n",
    "    if status == 'SUCCESS':\n",
    "        success_set.add(entry)\n",
    "    elif status == 'FAILED':\n",
    "        failed_set.add(entry)\n",
    "\n",
    "# Load files\n",
    "print(\"Loading JSON and CSV files...\")\n",
    "files = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
    "\n",
    "# Load type codes\n",
    "print(\"Loading type codes from CSV file...\")\n",
    "type_code_pd = pd.read_csv(r\"D:\\Desktop\\gaode\\type_code.csv\")\n",
    "print(\"Type codes loaded successfully.\")\n",
    "# Iterate through each JSON file to process the data\n",
    "for file in files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    with open(os.path.join(data_dir, file), 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} entries from {file}.\")\n",
    "\n",
    "    # Match type_code_str in type_code_pd\n",
    "    type_code_str = file.split('_')[0]\n",
    "    if type_code_str.startswith('0'):\n",
    "        type_code_str = type_code_str[1:]\n",
    "    print(f\"Matching type code: {type_code_str}\")\n",
    "    type_name = type_code_pd[type_code_pd['NEW_TYPE'] == int(type_code_str)]['Sub Category'].tolist()\n",
    "    \n",
    "    # Ensure the type code is valid\n",
    "    if not type_name:\n",
    "        print(f\"Error: Type code {type_code_str} not found in type codes.\")\n",
    "        continue\n",
    "\n",
    "    type_name = type_name[0]\n",
    "    print(f\"Type name matched: {type_name}\")\n",
    "\n",
    "    # Create relative directory for type\n",
    "    relative_dir = os.path.join(data_dir, type_name)\n",
    "    if not os.path.exists(relative_dir):\n",
    "        os.makedirs(relative_dir)\n",
    "        print(f\"Created directory: {relative_dir}\")\n",
    "    \n",
    "\n",
    "    # Iterate through data to extract information\n",
    "    for i in range(len(data)):\n",
    "        entry_name = data[i]['name']\n",
    "        if entry_name in success_set:\n",
    "            print(f\"Skipping successful entry: {entry_name}\")\n",
    "            continue\n",
    "\n",
    "        # Create directory for each shop\n",
    "        i_dir = os.path.join(relative_dir, entry_name)\n",
    "        if not os.path.exists(i_dir):\n",
    "            os.makedirs(i_dir)\n",
    "            print(f\"Created directory for shop: {i_dir}\")\n",
    "\n",
    "        search_command = [\n",
    "            \"python\", \"D:\\Desktop\\Coding\\MediaCrawler\\main.py\",\n",
    "            \"--platform\", \"xhs\",\n",
    "            \"--lt\", \"qrcode\",\n",
    "            \"--type\", \"search\",\n",
    "            \"--keywords\", f\"武汉{entry_name}\"\n",
    "        ]\n",
    "        print(f\"Executing command: {' '.join(search_command)}\")\n",
    "        try:\n",
    "            result = subprocess.run(search_command, check=True, text=True, capture_output=True)\n",
    "            print(result.stdout)\n",
    "            with open(log_file, 'a') as lf:\n",
    "                lf.write(f\"SUCCESS,{entry_name}\\n\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: {e}. Please ensure that the command and paths are correct.\")\n",
    "            with open(log_file, 'a') as lf:\n",
    "                lf.write(f\"FAILED,{entry_name}\\n\")\n",
    "            continue  # Continue processing next entry\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"命令执行失败，错误信息: {e}。请检查命令输出以获取详细信息。\")\n",
    "            print(f\"错误输出: {e.stderr}\")\n",
    "            \n",
    "            # 检查错误信息中是否包含 '出现验证码'\n",
    "            if '出现验证码' in e.stderr:\n",
    "                # 尝试从错误信息中提取数据\n",
    "                lines = e.stderr.split('\\n')\n",
    "                for line in lines:\n",
    "                    if 'Search notes res:' in line:\n",
    "                        # 提取 'Search notes res:' 后面的数据\n",
    "                        idx = line.find('Search notes res:')\n",
    "                        json_str = line[idx + len('Search notes res:'):].strip()\n",
    "                        \n",
    "                        # 打印提取的 json_str 以检查内容\n",
    "                        print(f\"提取的 JSON 字符串: {json_str}\")\n",
    "                        \n",
    "                        # 如果 json_str 不以 '{' 开头，找到第一个 '{' 的位置\n",
    "                        if not json_str.startswith('{'):\n",
    "                            brace_idx = json_str.find('{')\n",
    "                            if brace_idx != -1:\n",
    "                                json_str = json_str[brace_idx:]\n",
    "                            else:\n",
    "                                print(\"未找到 '{'，无法解析 JSON 数据。\")\n",
    "                                break  # 退出循环\n",
    "                        try:\n",
    "                            # 使用 ast.literal_eval 解析数据\n",
    "                            crawled_data = ast.literal_eval(json_str)\n",
    "                            \n",
    "                            # 如果需要保存为标准 JSON 格式，可以使用 json.dumps()\n",
    "                            json_data = json.dumps(crawled_data, ensure_ascii=False, indent=4)\n",
    "                            \n",
    "                            # 确保 JSON 目录存在\n",
    "                            json_dest = os.path.join(i_dir, 'json')\n",
    "                            if not os.path.exists(json_dest):\n",
    "                                os.makedirs(json_dest)\n",
    "                                print(f\"创建 JSON 目录: {json_dest}\")\n",
    "                            \n",
    "                            # 将 JSON 数据保存到文件\n",
    "                            json_filename = os.path.join(json_dest, f\"{entry_name}.json\")\n",
    "                            with open(json_filename, 'w', encoding='utf-8') as json_file:\n",
    "                                json_file.write(json_data)\n",
    "                            print(f\"已将 JSON 数据保存到 {json_filename}\")\n",
    "                        except Exception as ex:\n",
    "                            print(f\"解析数据失败: {ex}\")\n",
    "                            print(f\"数据内容: {json_str}\")\n",
    "                        break  # 处理完毕，退出循环\n",
    "            else:\n",
    "                print(\"错误信息中未找到验证码提示。\")\n",
    "            \n",
    "            # 记录失败日志并继续处理下一个商铺\n",
    "            with open(log_file, 'a') as lf:\n",
    "                lf.write(f\"FAILED,{entry_name}\\n\")\n",
    "            continue  # 继续处理下一个商铺\n",
    "\n",
    "\n",
    "            # Move search results to the corresponding folder\n",
    "        images_src = os.path.join(xhs_data_dir, 'images')\n",
    "        json_src = os.path.join(xhs_data_dir, 'json')\n",
    "        images_dest = os.path.join(i_dir, 'images')\n",
    "        json_dest = os.path.join(i_dir, 'json')\n",
    "        \n",
    "        if not os.path.exists(images_dest):\n",
    "            os.makedirs(images_dest)\n",
    "            print(f\"Created images directory: {images_dest}\")\n",
    "        if not os.path.exists(json_dest):\n",
    "            os.makedirs(json_dest)\n",
    "            print(f\"Created JSON directory: {json_dest}\")\n",
    "        \n",
    "        # Move images and JSON results\n",
    "        for img_file in os.listdir(images_src):\n",
    "            print(f\"Moving image file: {img_file} to {images_dest}\")\n",
    "            shutil.move(os.path.join(images_src, img_file), images_dest)\n",
    "        for json_file in os.listdir(json_src):\n",
    "            print(f\"Moving JSON file: {json_file} to {json_dest}\")\n",
    "            shutil.move(os.path.join(json_src, json_file), json_dest)\n",
    "\n",
    "\n",
    "# Visualization of Ratings\n",
    "print(\"Starting visualization of ratings...\")\n",
    "all_summaries = []\n",
    "for folder in os.listdir(data_dir):\n",
    "    folder_path = os.path.join(data_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        summary_file = os.path.join(folder_path, f\"{folder}_summary.csv\")\n",
    "        if os.path.exists(summary_file):\n",
    "            print(f\"Loading summary file: {summary_file}\")\n",
    "            all_summaries.append(pd.read_csv(summary_file))\n",
    "\n",
    "if all_summaries:\n",
    "    print(\"Combining all summary data...\")\n",
    "    combined_summary = pd.concat(all_summaries, ignore_index=True)\n",
    "    combined_summary['rating_gaode'] = pd.to_numeric(combined_summary['rating_gaode'], errors='coerce')\n",
    "\n",
    "    print(\"Plotting histogram of Gaode ratings...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(combined_summary['rating_gaode'].dropna(), bins=20, edgecolor='black')\n",
    "    plt.xlabel('Gaode Rating')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Gaode Ratings')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Process completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
